# apache-airflow-spark

Here in this repository, we have designed a simple ETL process that extract data from an API and we are transforming this data using Spark and loading this data into a Delta table sitting on AWS S3 bucket. We running this batch processes using the Spark job submit in Airflow. 

We are using the API data provided by 

## Things to do;

*  Set up Apache Spark locally. 
*  Write the Spark Jobs to Extract, Transform and Load the data. 
*  Set up Apache Airflow on locally.
*  Design the Airflow DAG to trigger the Spark Jobs.



